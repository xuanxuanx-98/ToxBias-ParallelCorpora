@inproceedings{sap-etal-2019-risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}


@article{gitari-2015-lexicon,
  title={A lexicon-based approach for hate speech detection},
  author={Gitari, Njagi Dennis and Zuping, Zhang and Damien, Hanyurwimfura and Long, Jun},
  journal={International Journal of Multimedia and Ubiquitous Engineering},
  volume={10},
  number={4},
  pages={215--230},
  year={2015}
}

@inproceedings{alrehili-2019-survey,
  author={Alrehili, Ahlam},
  booktitle={2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)},
  title={Automatic Hate Speech Detection on Social Media: A Brief Survey},
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Dictionaries;Sentiment analysis;Feature extraction;Machine learning;Facebook;hate speech;OSN;online social network;automatic detection;offensive;hateful dictionaries;bag-of-words;N-gram},
  doi={10.1109/AICCSA47632.2019.9035228}}

@inproceedings{sap-etal-2019-risk,
  title = "The Risk of Racial Bias in Hate Speech Detection",
  author = "Sap, Maarten  and
    Card, Dallas  and
    Gabriel, Saadia  and
    Choi, Yejin  and
    Smith, Noah A.",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1163",
  doi = "10.18653/v1/P19-1163",
  pages = "1668--1678",
  abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}

@inproceedings{zhou-etal-2021-challenges,
    title = "Challenges in Automated Debiasing for Toxic Language Detection",
    author = "Zhou, Xuhui  and
      Sap, Maarten  and
      Swayamdipta, Swabha  and
      Choi, Yejin  and
      Smith, Noah",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.274",
    doi = "10.18653/v1/2021.eacl-main.274",
    pages = "3143--3155",
    abstract = "Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.",
}

@misc{lees-2022-new-perspective,
      title={A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
      author={Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
      year={2022},
      eprint={2202.11176},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{google-perspective,
  author       = {Google},
  title        = {Perspective API},
  year         = {2024},
  url          = {https://perspectiveapi.com/},
  note         = {Accessed: 2024-06-19}
}

@inproceedings{davidson-etal-2019-racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@article{blodgett-2017-racial,
  title={Racial disparity in natural language processing: A case study of social media african-american english},
  author={Blodgett, Su Lin and O'Connor, Brendan},
  journal={arXiv preprint arXiv:1707.00061},
  year={2017}
}

@inproceedings{xia-etal-2020-demoting,
    title = "Demoting Racial Bias in Hate Speech Detection",
    author = "Xia, Mengzhou  and
      Field, Anjalie  and
      Tsvetkov, Yulia",
    editor = "Ku, Lun-Wei  and
      Li, Cheng-Te",
    booktitle = "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.socialnlp-1.2",
    doi = "10.18653/v1/2020.socialnlp-1.2",
    pages = "7--14",
    abstract = "In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators{'} perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.",
}

@article{schafer-2023-bias,
  title={Bias Mitigation for Capturing Potentially Illegal Hate Speech},
  author={Sch{\"a}fer, Johannes},
  journal={Datenbank-Spektrum},
  volume={23},
  number={1},
  pages={41--51},
  year={2023},
  publisher={Springer}
}

@article{resende-2024-comprehensive,
  title={A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions},
  author={Resende, Guilherme H and Nery, Luiz F and Benevenuto, Fabr{\'\i}cio and Zannettou, Savvas and Figueiredo, Flavio},
  journal={arXiv preprint arXiv:2401.12720},
  year={2024}
}

@article{zhao-2022-subjectivity,
  title = {Utilizing subjectivity level to mitigate identity term bias in toxic comments classification},
  journal = {Online Social Networks and Media},
  volume = {29},
  pages = {100205},
  year = {2022},
  issn = {2468-6964},
  doi = {https://doi.org/10.1016/j.osnem.2022.100205},
  url = {https://www.sciencedirect.com/science/article/pii/S246869642200009X},
  author = {Zhixue Zhao and Ziqi Zhang and Frank Hopfgartner},
  keywords = {Language model, Transfer learning, Hate speech, Classification},
  abstract = {Toxic comment classification models are often found biased towards identity terms, i.e., terms characterizing a specific group of people such as “Muslim” and “black”. Such bias is commonly reflected in false positive predictions, i.e., non-toxic comments with identity terms. In this work, we propose a novel approach to debias the model in toxic comment classification, leveraging the notion of subjectivity level of a comment and the presence of identity terms. We hypothesize that toxic comments containing identity terms are more likely to be expressions of subjective feelings or opinions. Therefore, the subjectivity level of a comment containing identity terms can be helpful for classifying toxic comments and mitigating the identity term bias. To implement this idea, we propose a model based on BERT and study two different methods of measuring the subjectivity level. The first method uses a lexicon-based tool. The second method is based on the idea of calculating the embedding similarity between a comment and a relevant Wikipedia text of the identity term in the comment. We thoroughly evaluate our method on an extensive collection of four datasets collected from different social media platforms. Our results show that: (1) our models that incorporate both features of subjectivity and identity terms consistently outperform strong SOTA baselines, with our best performing model achieving an improvement in F1 of 4.75% over a Twitter dataset; (2) our idea of measuring subjectivity based on the similarity to the relevant Wikipedia text is very effective on toxic comment classification as our model using this has achieved the best performance on 3 out of 4 datasets while obtaining comparative performance on the remaining dataset. We further test our method on RoBERTa to evaluate the generality of our method and the results show the biggest improvement in F1 of up to 1.29% (on a dataset from a white supremacist online forum).}
}

@inproceedings{ball-2021-differential,
  title={Differential tweetment: Mitigating racial dialect bias in harmful tweet detection},
  author={Ball-Burack, Ari and Lee, Michelle Seng Ah and Cobbe, Jennifer and Singh, Jatinder},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={116--128},
  year={2021}
}

@inproceedings{okpala-2022-aaebert,
  title={AAEBERT: Debiasing BERT-based Hate Speech Detection Models via Adversarial Learning},
  author={Okpala, Ebuka and Cheng, Long and Mbwambo, Nicodemus and Luo, Feng},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={1606--1612},
  year={2022},
  organization={IEEE}
}

@inproceedings{mathew-2021-hatexplain,
  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={17},
  pages={14867--14875},
  year={2021}
}

@inproceedings{ziems-2022-value,
    title = "{VALUE}: {U}nderstanding Dialect Disparity in {NLU}",
    author = "Ziems, Caleb  and
      Chen, Jiaao  and
      Harris, Camille  and
      Anderson, Jessica  and
      Yang, Diyi",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.258",
    doi = "10.18653/v1/2022.acl-long.258",
    pages = "3701--3720",
    abstract = "English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE. However, these benchmarks contain only textbook Standard American English (SAE). Other dialects have been largely overlooked in the NLP community. This leads to biased and inequitable NLU systems that serve only a sub-population of speakers. To understand disparities in current models and to facilitate more dialect-competent NLU systems, we introduce the VernAcular Language Understanding Evaluation (VALUE) benchmark, a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules. In this initial release (V.1), we construct rules for 11 features of African American Vernacular English (AAVE), and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner. Experiments show that these new dialectal features can lead to a drop in model performance.",
}

@inproceedings{ziems-2023-multi,
    title = "Multi-{VALUE}: A Framework for Cross-Dialectal {E}nglish {NLP}",
    author = "Ziems, Caleb  and
      Held, William  and
      Yang, Jingfeng  and
      Dhamala, Jwala  and
      Gupta, Rahul  and
      Yang, Diyi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.44",
    doi = "10.18653/v1/2023.acl-long.44",
    pages = "744--768",
    abstract = "Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve the dialect robustness of existing systems. Finally, we partner with native speakers of Chicano and Indian English to release new gold-standard variants of the popular CoQA task. To execute the transformation code, run model checkpoints, and download both synthetic and gold-standard dialectal benchmark datasets, see \url{http://value-nlp.org}.",
}


@Inbook{dash-2019-scarcity,
author="Dash, Niladri Sekhar
and Ramamoorthy, L.",
title="Corpus and Dialect Study",
bookTitle="Utility and Application of Language Corpora ",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="139--153",
abstract="In the present Indian contextContext, we find that many minority language communities are living in different sociocultural and geoclimatic regions across the country. Any kind of systematic study on these languages requires well-formed and properly representative dialect corpusDialect corpusin digital form because a dialect corpusDialect corpusdue to its overall representationRepresentationof the dialectDialectin question is the most reliable resource for studying the dialectsDialect wordin a faithful manner. It is the dialect corpusDialect corpus, and not a general corpusCorpusof a standard language, and is of primary importance here, as a dialect corpusDialect corpusis a unique kind of resource that can supply the most authentic data and informationInformationof dialectDialectvariation that can be investigated with empirical details and verifiable authenticityAuthenticity. Keeping this aspect in view, in this chapterChapter, we shall try to discuss how the study of dialects can be more rational, reliable, authentic, and useful if we carry out research on dialects with the direct utilizationUtilizationof dialect corpusDialect corpusdeveloped in digital form. In this case, the modern dialectologists are not bound to depend, like traditional dialectologists, on data, examplesExamples, and informationInformationelicited in a controlled manner by analyzing responsesResponseselicited from a set of selected informants against the questions asked to them. In this new method, we argue that if we can develop a full-fledged large, multidimensional, and widely representative dialect corpusDialect corpusfollowing the methods, methods and strategies used in modern corpus linguisticsCorpus linguistics, a dialect corpusDialect corpuswill be more rational in demographic samplingDemographic sampling, more reliable in text representationText representation, more authentic in linguistic observationLinguistic observation, and more verifiable inCommunity developmentinference deductionInference deduction.",
isbn="978-981-13-1801-6",
doi="10.1007/978-981-13-1801-6_9",
url="https://doi.org/10.1007/978-981-13-1801-6_9"
}

@inproceedings{jorgensen-2015-challenges,
  title={Challenges of studying and processing dialects in social media},
  author={J{\o}rgensen, Anna and Hovy, Dirk and S{\o}gaard, Anders},
  booktitle={Proceedings of the workshop on noisy user-generated text},
  pages={9--18},
  year={2015}
}

@inproceedings{bird-2020-augmentation,
  author={Bird, Jordan J. and Faria, Diego R. and Premebida, Cristiano and Ekárt, Anikó and Ayrosa, Pedro P. S.},
  booktitle={2020 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  title={Overcoming Data Scarcity in Speaker Identification: Dataset Augmentation with Synthetic MFCCs via Character-level RNN},
  year={2020},
  volume={},
  number={},
  pages={146-151},
  keywords={Data models;Mel frequency cepstral coefficient;Recurrent neural networks;Speech recognition;Speaker recognition;Human-robot interaction;Autonomous systems;Data Augmentation;Speaker Identification;Speech Recognition;Generative Models;Human-robot Interaction;Autonomous Systems},
  doi={10.1109/ICARSC49921.2020.9096166}}

@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",
}

@misc{jigsaw-2017-perspective,
  author = {Jigsaw},
  title = {Perspective API: Using Machine Learning to Reduce Online Harassment},
  year = {2017},
  month = {February},
  url = {https://jigsaw.google.com/news/announcement/perspective-api-launch},
  note = {Accessed: 2024-06-28}
}









