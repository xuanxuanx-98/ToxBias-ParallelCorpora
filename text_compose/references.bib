@inproceedings{sap-etal-2019-risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}


@article{gitari-2015-lexicon,
  title={A lexicon-based approach for hate speech detection},
  author={Gitari, Njagi Dennis and Zuping, Zhang and Damien, Hanyurwimfura and Long, Jun},
  journal={International Journal of Multimedia and Ubiquitous Engineering},
  volume={10},
  number={4},
  pages={215--230},
  year={2015}
}

@inproceedings{alrehili-2019-survey,
  author={Alrehili, Ahlam},
  booktitle={2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)},
  title={Automatic Hate Speech Detection on Social Media: A Brief Survey},
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Dictionaries;Sentiment analysis;Feature extraction;Machine learning;Facebook;hate speech;OSN;online social network;automatic detection;offensive;hateful dictionaries;bag-of-words;N-gram},
  doi={10.1109/AICCSA47632.2019.9035228}}

@inproceedings{sap-etal-2019-risk,
  title = "The Risk of Racial Bias in Hate Speech Detection",
  author = "Sap, Maarten  and
    Card, Dallas  and
    Gabriel, Saadia  and
    Choi, Yejin  and
    Smith, Noah A.",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1163",
  doi = "10.18653/v1/P19-1163",
  pages = "1668--1678",
  abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}

@inproceedings{zhou-etal-2021-challenges,
    title = "Challenges in Automated Debiasing for Toxic Language Detection",
    author = "Zhou, Xuhui  and
      Sap, Maarten  and
      Swayamdipta, Swabha  and
      Choi, Yejin  and
      Smith, Noah",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.274",
    doi = "10.18653/v1/2021.eacl-main.274",
    pages = "3143--3155",
    abstract = "Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.",
}

@misc{lees-2022-new-perspective,
      title={A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
      author={Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
      year={2022},
      eprint={2202.11176},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{google-perspective,
  author       = {Google},
  title        = {Perspective API},
  year         = {2024},
  url          = {https://perspectiveapi.com/},
  note         = {Accessed: 2024-06-19}
}

@inproceedings{davidson-etal-2019-racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@article{blodgett-2017-racial,
  title={Racial disparity in natural language processing: A case study of social media african-american english},
  author={Blodgett, Su Lin and O'Connor, Brendan},
  journal={arXiv preprint arXiv:1707.00061},
  year={2017}
}

@inproceedings{xia-etal-2020-demoting,
    title = "Demoting Racial Bias in Hate Speech Detection",
    author = "Xia, Mengzhou  and
      Field, Anjalie  and
      Tsvetkov, Yulia",
    editor = "Ku, Lun-Wei  and
      Li, Cheng-Te",
    booktitle = "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.socialnlp-1.2",
    doi = "10.18653/v1/2020.socialnlp-1.2",
    pages = "7--14",
    abstract = "In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators{'} perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.",
}

@article{schafer-2023-bias,
  title={Bias Mitigation for Capturing Potentially Illegal Hate Speech},
  author={Sch{\"a}fer, Johannes},
  journal={Datenbank-Spektrum},
  volume={23},
  number={1},
  pages={41--51},
  year={2023},
  publisher={Springer}
}

@article{resende-2024-comprehensive,
  title={A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions},
  author={Resende, Guilherme H and Nery, Luiz F and Benevenuto, Fabr{\'\i}cio and Zannettou, Savvas and Figueiredo, Flavio},
  journal={arXiv preprint arXiv:2401.12720},
  year={2024}
}

@article{zhao-2022-subjectivity,
  title = {Utilizing subjectivity level to mitigate identity term bias in toxic comments classification},
  journal = {Online Social Networks and Media},
  volume = {29},
  pages = {100205},
  year = {2022},
  issn = {2468-6964},
  doi = {https://doi.org/10.1016/j.osnem.2022.100205},
  url = {https://www.sciencedirect.com/science/article/pii/S246869642200009X},
  author = {Zhixue Zhao and Ziqi Zhang and Frank Hopfgartner},
  keywords = {Language model, Transfer learning, Hate speech, Classification},
  abstract = {Toxic comment classification models are often found biased towards identity terms, i.e., terms characterizing a specific group of people such as “Muslim” and “black”. Such bias is commonly reflected in false positive predictions, i.e., non-toxic comments with identity terms. In this work, we propose a novel approach to debias the model in toxic comment classification, leveraging the notion of subjectivity level of a comment and the presence of identity terms. We hypothesize that toxic comments containing identity terms are more likely to be expressions of subjective feelings or opinions. Therefore, the subjectivity level of a comment containing identity terms can be helpful for classifying toxic comments and mitigating the identity term bias. To implement this idea, we propose a model based on BERT and study two different methods of measuring the subjectivity level. The first method uses a lexicon-based tool. The second method is based on the idea of calculating the embedding similarity between a comment and a relevant Wikipedia text of the identity term in the comment. We thoroughly evaluate our method on an extensive collection of four datasets collected from different social media platforms. Our results show that: (1) our models that incorporate both features of subjectivity and identity terms consistently outperform strong SOTA baselines, with our best performing model achieving an improvement in F1 of 4.75% over a Twitter dataset; (2) our idea of measuring subjectivity based on the similarity to the relevant Wikipedia text is very effective on toxic comment classification as our model using this has achieved the best performance on 3 out of 4 datasets while obtaining comparative performance on the remaining dataset. We further test our method on RoBERTa to evaluate the generality of our method and the results show the biggest improvement in F1 of up to 1.29% (on a dataset from a white supremacist online forum).}
}

@inproceedings{ball-2021-differential,
  title={Differential tweetment: Mitigating racial dialect bias in harmful tweet detection},
  author={Ball-Burack, Ari and Lee, Michelle Seng Ah and Cobbe, Jennifer and Singh, Jatinder},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={116--128},
  year={2021}
}

@inproceedings{okpala-2022-aaebert,
  title={AAEBERT: Debiasing BERT-based Hate Speech Detection Models via Adversarial Learning},
  author={Okpala, Ebuka and Cheng, Long and Mbwambo, Nicodemus and Luo, Feng},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={1606--1612},
  year={2022},
  organization={IEEE}
}







